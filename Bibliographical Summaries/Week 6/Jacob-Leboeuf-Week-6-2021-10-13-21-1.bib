@article{edseee.711513120160401,
Abstract = {In this paper, we present a technique for recovering a model of shape, illumination, reflectance, and shading from a single image taken from an RGB-D sensor. To do this, we extend the SIRFS (“shape, illumination and reflectance from shading”) model, which recovers intrinsic scene properties from a single image [1] . Though SIRFS works well on neatly segmented images of objects, it performs poorly on images of natural scenes which often contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we model a scene using a mixture of shapes and a mixture of illuminations, where those mixture components are embedded in a “soft” segmentation-like representation of the input image. We use the noisy depth maps provided by RGB-D sensors (such as the Microsoft Kinect) to guide and improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflecta},
Author = {Barron, J.T. and Malik, J.},
ISSN = {0162-8828},
Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence, Pattern Analysis and Machine Intelligence, IEEE Transactions on, IEEE Trans. Pattern Anal. Mach. Intell},
Keywords = {Computing and Processing, Bioengineering, Shape, Lighting, Image segmentation, Rendering (computer graphics), Computational modeling, Noise, Noise measurement, depth sensing, computer vision, machine learning, intrinsic images, shape from shading, shape estimation, illumination estimation, segmentation, normalized cuts},
Number = {4},
Pages = {690 - 703},
Title = {Intrinsic Scene Properties from a Single RGB-D Image.},
Volume = {38},
URL = {https://umasslowell.idm.oclc.org/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=edseee&AN=edseee.7115131&site=eds-live},
Year = {2016},
}